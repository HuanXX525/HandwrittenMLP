{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播的实现\n",
    "\n",
    "- 根据链式法则求所有变量的偏导（一元内相乘，多元间相加）\n",
    "  $$\n",
    "    \\frac{\\partial L}{\\partial x} = \\sum_i \\frac{\\partial L}{\\partial y_i}\\frac{\\partial y_i}{\\partial x}\n",
    "  $$\n",
    "- 局部梯度的计算需要知道区分运算，还需要知道运算数\n",
    "- 必须自己定义一个类用运算符重载和属性实现这些功能（Value）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backward import Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图绘制函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._child:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='pdf', rankdir='LR', filename='computation_graph'):\n",
    "    \"\"\"\n",
    "    输出计算图为 PDF 文件\n",
    "    :param root: 计算图根节点（如 loss Value 对象）\n",
    "    :param format: 输出格式，设为 'pdf' 即可\n",
    "    :param rankdir: 布局方向（LR=左右，TB=上下）\n",
    "    :param filename: 输出文件名（无需加 .pdf 后缀）\n",
    "    :return: Digraph 对象，调用 render() 生成文件\n",
    "    \"\"\"\n",
    "    print(f\"Drawing graph in format: {format}, rankdir: {rankdir}\")\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    print(f\"Nodes: {len(nodes)}, Edges: {len(edges)}\")\n",
    "    \n",
    "    # 关键修改：format 设为 'pdf'\n",
    "    dot = Digraph(\n",
    "        format=format,\n",
    "        graph_attr={'rankdir': rankdir},\n",
    "        node_attr={'shape': 'record'},  # 固定节点形状为 record\n",
    "        filename=filename  # 指定输出文件名（后续无需重复写）\n",
    "    )\n",
    "    \n",
    "    for n in nodes:\n",
    "        # 节点标签：显示 data 和 grad（保留4位小数）\n",
    "        dot.node(\n",
    "            name=str(id(n)),\n",
    "            label=f\"{{ data: {n.data:.4f} | grad: {n.grad:.4f} }}\"\n",
    "        )\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        # 绘制节点间的边（子节点 → 父节点，符合计算图方向）\n",
    "        dot.edge(str(id(n1)), str(id(n2)))\n",
    "    \n",
    "    # 生成 PDF 文件（默认保存在当前目录）\n",
    "    dot.render(filename=filename, view=False)  # view=True 会自动打开 PDF\n",
    "    print(f\"PDF 文件已保存为：{filename}.pdf\")\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "add 节点 2065752981824 的子节点：[2065752849824, 2065752850160]\n",
      "add 节点 2065752982064 的子节点：[2065752981824, 2065752981632]\n",
      "add 节点 2065752982304 的子节点：[2065752982064, 2065223308096]\n",
      "add 节点 2065752983552 的子节点：[2065752982640, 2065752982976]\n",
      "add 节点 2065752983792 的子节点：[2065752983552, 2065752983312]\n",
      "add 节点 2065752984032 的子节点：[2065752983792, 2065752700304]\n",
      "add 节点 2065752985280 的子节点：[2065752984368, 2065752984704]\n",
      "add 节点 2065752985520 的子节点：[2065752985280, 2065752985040]\n",
      "add 节点 2065752985760 的子节点：[2065752985520, 2065752225744]\n",
      "add 节点 2065752987680 的子节点：[2065752986960, 2065752987200]\n",
      "add 节点 2065752987920 的子节点：[2065752987680, 2065752987440]\n",
      "add 节点 2065752988160 的子节点：[2065752987920, 2065752840560]\n",
      "add 节点 2065752989120 的子节点：[2065752988400, 2065752988640]\n",
      "add 节点 2065752989360 的子节点：[2065752989120, 2065752988880]\n",
      "add 节点 2065752989600 的子节点：[2065752989360, 2065752848672]\n",
      "add 节点 2065752990560 的子节点：[2065752989840, 2065752990080]\n",
      "add 节点 2065752990800 的子节点：[2065752990560, 2065752990320]\n",
      "add 节点 2065752991040 的子节点：[2065752990800, 2065752848816]\n",
      "add 节点 2065752992000 的子节点：[2065752991280, 2065752991520]\n",
      "add 节点 2065752992240 的子节点：[2065752992000, 2065752991760]\n",
      "add 节点 2065752992480 的子节点：[2065752992240, 2065752849200]\n",
      "add 节点 2065752993440 的子节点：[2065752992720, 2065752992960]\n",
      "add 节点 2065752993680 的子节点：[2065752993440, 2065752993200]\n",
      "add 节点 2065752993920 的子节点：[2065752993680, 2065752849632]\n",
      "add 节点 2065752994256 的子节点：[2065752988160, 2065752994064]\n",
      "add 节点 2065752994784 的子节点：[2065752989600, 2065752994592]\n",
      "add 节点 2065752995360 的子节点：[2065752991040, 2065752995168]\n",
      "add 节点 2065752995936 的子节点：[2065752992480, 2065752995744]\n",
      "add 节点 2065752996512 的子节点：[2065752993920, 2065752996320]\n",
      "add 节点 2065752996992 的子节点：[2065752299936, 2065752995024]\n",
      "add 节点 2065752997232 的子节点：[2065752996992, 2065752995600]\n",
      "add 节点 2065752997472 的子节点：[2065752997232, 2065752996176]\n",
      "add 节点 2065752997712 的子节点：[2065752997472, 2065752996752]\n",
      "losses2:1.6094379124341003:   0.00,   0.00,   1.61,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00\n",
      "avarage loss: 0.16094379124341002\n",
      "后序遍历添加节点：2065752848528\n",
      "后序遍历添加节点：2065244751632\n",
      "后序遍历添加节点：2065752700976\n",
      "后序遍历添加节点：2065752849824\n",
      "后序遍历添加节点：2065244470592\n",
      "后序遍历添加节点：2065752849968\n",
      "后序遍历添加节点：2065752850160\n",
      "后序遍历添加节点：2065752981824\n",
      "后序遍历添加节点：2065244459792\n",
      "后序遍历添加节点：2065752850304\n",
      "后序遍历添加节点：2065752981632\n",
      "后序遍历添加节点：2065752982064\n",
      "后序遍历添加节点：2065223308096\n",
      "后序遍历添加节点：2065752982304\n",
      "后序遍历添加节点：2065752985952\n",
      "后序遍历添加节点：2065752986048\n",
      "后序遍历添加节点：2065752989840\n",
      "后序遍历添加节点：2065752848720\n",
      "后序遍历添加节点：2065752688304\n",
      "后序遍历添加节点：2065752982448\n",
      "后序遍历添加节点：2065752982640\n",
      "后序遍历添加节点：2065752693920\n",
      "后序遍历添加节点：2065752982784\n",
      "后序遍历添加节点：2065752982976\n",
      "后序遍历添加节点：2065752983552\n",
      "后序遍历添加节点：2065752689072\n",
      "后序遍历添加节点：2065752983120\n",
      "后序遍历添加节点：2065752983312\n",
      "后序遍历添加节点：2065752983792\n",
      "后序遍历添加节点：2065752700304\n",
      "后序遍历添加节点：2065752984032\n",
      "后序遍历添加节点：2065752986288\n",
      "后序遍历添加节点：2065752986384\n",
      "后序遍历添加节点：2065752990080\n",
      "后序遍历添加节点：2065752990560\n",
      "后序遍历添加节点：2065752848768\n",
      "后序遍历添加节点：2065241127536\n",
      "后序遍历添加节点：2065752984176\n",
      "后序遍历添加节点：2065752984368\n",
      "后序遍历添加节点：2065241113664\n",
      "后序遍历添加节点：2065752984512\n",
      "后序遍历添加节点：2065752984704\n",
      "后序遍历添加节点：2065752985280\n",
      "后序遍历添加节点：2065752225456\n",
      "后序遍历添加节点：2065752984848\n",
      "后序遍历添加节点：2065752985040\n",
      "后序遍历添加节点：2065752985520\n",
      "后序遍历添加节点：2065752225744\n",
      "后序遍历添加节点：2065752985760\n",
      "后序遍历添加节点：2065752986624\n",
      "后序遍历添加节点：2065752986720\n",
      "后序遍历添加节点：2065752990320\n",
      "后序遍历添加节点：2065752990800\n",
      "后序遍历添加节点：2065752848816\n",
      "后序遍历添加节点：2065752991040\n",
      "后序遍历添加节点：2065752995168\n",
      "后序遍历添加节点：2065752995360\n",
      "后序遍历添加节点：2065752995600\n",
      "后序遍历添加节点：2065244550064\n",
      "后序遍历添加节点：2065752986960\n",
      "后序遍历添加节点：2065752838352\n",
      "后序遍历添加节点：2065752987200\n",
      "后序遍历添加节点：2065752987680\n",
      "后序遍历添加节点：2065752840320\n",
      "后序遍历添加节点：2065752987440\n",
      "后序遍历添加节点：2065752987920\n",
      "后序遍历添加节点：2065752840560\n",
      "后序遍历添加节点：2065752988160\n",
      "后序遍历添加节点：2065752994064\n",
      "后序遍历添加节点：2065752994256\n",
      "后序遍历添加节点：2065752299936\n",
      "后序遍历添加节点：2065752840032\n",
      "后序遍历添加节点：2065752988400\n",
      "后序遍历添加节点：2065752842048\n",
      "后序遍历添加节点：2065752988640\n",
      "后序遍历添加节点：2065752989120\n",
      "后序遍历添加节点：2065752841856\n",
      "后序遍历添加节点：2065752988880\n",
      "后序遍历添加节点：2065752989360\n",
      "后序遍历添加节点：2065752848672\n",
      "后序遍历添加节点：2065752989600\n",
      "后序遍历添加节点：2065752994592\n",
      "后序遍历添加节点：2065752994784\n",
      "后序遍历添加节点：2065752995024\n",
      "后序遍历添加节点：2065752996992\n",
      "后序遍历添加节点：2065752997232\n",
      "后序遍历添加节点：2065752848912\n",
      "后序遍历添加节点：2065752991280\n",
      "后序遍历添加节点：2065752849008\n",
      "后序遍历添加节点：2065752991520\n",
      "后序遍历添加节点：2065752992000\n",
      "后序遍历添加节点：2065752849104\n",
      "后序遍历添加节点：2065752991760\n",
      "后序遍历添加节点：2065752992240\n",
      "后序遍历添加节点：2065752849200\n",
      "后序遍历添加节点：2065752992480\n",
      "后序遍历添加节点：2065752995744\n",
      "后序遍历添加节点：2065752995936\n",
      "后序遍历添加节点：2065752996176\n",
      "后序遍历添加节点：2065752997472\n",
      "后序遍历添加节点：2065752849344\n",
      "后序遍历添加节点：2065752992720\n",
      "后序遍历添加节点：2065752849440\n",
      "后序遍历添加节点：2065752992960\n",
      "后序遍历添加节点：2065752993440\n",
      "后序遍历添加节点：2065752849536\n",
      "后序遍历添加节点：2065752993200\n",
      "后序遍历添加节点：2065752993680\n",
      "后序遍历添加节点：2065752849632\n",
      "后序遍历添加节点：2065752993920\n",
      "后序遍历添加节点：2065752996320\n",
      "后序遍历添加节点：2065752996512\n",
      "后序遍历添加节点：2065752996752\n",
      "后序遍历添加节点：2065752997712\n",
      "后序遍历添加节点：2065753015456\n",
      "后序遍历添加节点：2065753015648\n",
      "后序遍历添加节点：2065753015888\n",
      "后序遍历添加节点：2065753017472\n",
      "后序遍历添加节点：2065750675472\n",
      "后序遍历添加节点：2065753017232\n",
      "后序遍历添加节点：2065753017808\n",
      "正确拓扑顺序（子→父）： [2065752848528, 2065244751632, 2065752700976, 2065752849824, 2065244470592, 2065752849968, 2065752850160, 2065752981824, 2065244459792, 2065752850304, 2065752981632, 2065752982064, 2065223308096, 2065752982304, 2065752985952, 2065752986048, 2065752989840, 2065752848720, 2065752688304, 2065752982448, 2065752982640, 2065752693920, 2065752982784, 2065752982976, 2065752983552, 2065752689072, 2065752983120, 2065752983312, 2065752983792, 2065752700304, 2065752984032, 2065752986288, 2065752986384, 2065752990080, 2065752990560, 2065752848768, 2065241127536, 2065752984176, 2065752984368, 2065241113664, 2065752984512, 2065752984704, 2065752985280, 2065752225456, 2065752984848, 2065752985040, 2065752985520, 2065752225744, 2065752985760, 2065752986624, 2065752986720, 2065752990320, 2065752990800, 2065752848816, 2065752991040, 2065752995168, 2065752995360, 2065752995600, 2065244550064, 2065752986960, 2065752838352, 2065752987200, 2065752987680, 2065752840320, 2065752987440, 2065752987920, 2065752840560, 2065752988160, 2065752994064, 2065752994256, 2065752299936, 2065752840032, 2065752988400, 2065752842048, 2065752988640, 2065752989120, 2065752841856, 2065752988880, 2065752989360, 2065752848672, 2065752989600, 2065752994592, 2065752994784, 2065752995024, 2065752996992, 2065752997232, 2065752848912, 2065752991280, 2065752849008, 2065752991520, 2065752992000, 2065752849104, 2065752991760, 2065752992240, 2065752849200, 2065752992480, 2065752995744, 2065752995936, 2065752996176, 2065752997472, 2065752849344, 2065752992720, 2065752849440, 2065752992960, 2065752993440, 2065752849536, 2065752993200, 2065752993680, 2065752849632, 2065752993920, 2065752996320, 2065752996512, 2065752996752, 2065752997712, 2065753015456, 2065753015648, 2065753015888, 2065753017472, 2065750675472, 2065753017232, 2065753017808]\n",
      "执行节点 2065753017808 的 _backward\n",
      "执行节点 2065750675472 的 _backward\n",
      "执行节点 2065753017472 的 _backward\n",
      "log 节点的 self id：2065753015888\n",
      "log 梯度计算：derivative=5.0, out.grad=-1.0, 累加值=-5.0\n",
      "执行节点 2065753015888 的 _backward\n",
      "执行节点 2065753015648 的 _backward\n",
      "执行节点 2065752997712 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752996752 的 _backward\n",
      "执行节点 2065752996512 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752993920 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752993680 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752993200 的 _backward\n",
      "执行节点 2065752993440 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752992960 的 _backward\n",
      "执行节点 2065752992720 的 _backward\n",
      "执行节点 2065752997472 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752996176 的 _backward\n",
      "执行节点 2065752995936 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752992480 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752992240 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752991760 的 _backward\n",
      "执行节点 2065752992000 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752991520 的 _backward\n",
      "执行节点 2065752991280 的 _backward\n",
      "执行节点 2065752997232 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752996992 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752995024 的 _backward\n",
      "执行节点 2065752994784 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752989600 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752989360 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752988880 的 _backward\n",
      "执行节点 2065752989120 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752988640 的 _backward\n",
      "执行节点 2065752988400 的 _backward\n",
      "执行节点 2065752299936 的 _backward\n",
      "执行节点 2065752994256 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752988160 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752987920 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752987440 的 _backward\n",
      "执行节点 2065752987680 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752987200 的 _backward\n",
      "执行节点 2065752986960 的 _backward\n",
      "执行节点 2065752995600 的 _backward\n",
      "执行节点 2065752995360 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752991040 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752990800 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752990320 的 _backward\n",
      "执行节点 2065752986720 的 _backward\n",
      "执行节点 2065752985760 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752985520 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752985040 的 _backward\n",
      "执行节点 2065752985280 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752984704 的 _backward\n",
      "执行节点 2065752984368 的 _backward\n",
      "执行节点 2065752990560 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752990080 的 _backward\n",
      "执行节点 2065752986384 的 _backward\n",
      "执行节点 2065752984032 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752983792 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752983312 的 _backward\n",
      "执行节点 2065752983552 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752982976 的 _backward\n",
      "执行节点 2065752982640 的 _backward\n",
      "执行节点 2065752989840 的 _backward\n",
      "执行节点 2065752986048 的 _backward\n",
      "执行节点 2065752982304 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752982064 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752981632 的 _backward\n",
      "执行节点 2065752981824 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065752850160 的 _backward\n",
      "执行节点 2065752849824 的 _backward\n",
      "Drawing graph in format: pdf, rankdir: LR\n",
      "Nodes: 121, Edges: 133\n",
      "PDF 文件已保存为：computation_graph.pdf\n",
      "Value(1.6094379124341003)\n"
     ]
    }
   ],
   "source": [
    "from backward import ActivationFunction, LossFunction, Optimizer, to_one_hot, Layer, SGDMomentum, Adam\n",
    "from data_loader import load_mnist_dataset\n",
    "import numpy as np\n",
    "numInputs, numOutputs, numHiddens = 3, 5, 3 # 定义模型参数\n",
    "\n",
    "L1 = Layer(numInputs, numHiddens)\n",
    "L2 = Layer(numHiddens, numOutputs)\n",
    "\n",
    "opti = Optimizer(L1.parameters()+L2.parameters(), 0.01)\n",
    "\n",
    "# train_images, train_labels, test_images, test_labels = load_mnist_dataset('./data', flatten=True)\n",
    "# train_images = np.array(train_images)\n",
    "train_images = np.array([[0, 0, 4], [5, 0, 1], [1, 5, 0]])\n",
    "\n",
    "train_labels = np.array([2, 0, 1])\n",
    "\n",
    "\n",
    "\n",
    "lossL = [0] * 10\n",
    "for epoch in range(1):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for imgIdx in range(1):\n",
    "\n",
    "        img = train_images[imgIdx]\n",
    "\n",
    "        label = train_labels[imgIdx]\n",
    "        # 前向传播\n",
    "        a1 = L1(img)\n",
    "        # print(f\"a1: {a1}\")\n",
    "        a2 = L2(ActivationFunction.RelU(a1))\n",
    "        # print(f\"a2: {a2}\")\n",
    "        yHat = ActivationFunction.softmax(a2)\n",
    "        # print(f\"yHat: {yHat}\")\n",
    "        oh = to_one_hot(label, 10)\n",
    "        # print(oh)\n",
    "        loss = LossFunction.categorical_cross_entropy(yHat, oh)\n",
    "        diff = loss.data - lossL[label]\n",
    "        lossL[label] = loss.data\n",
    "        formatted_losses = [f\"{loss:6.2f}\" for loss in lossL]\n",
    "        print(f\"losses{label}:{diff}: {', '.join(formatted_losses)}\")\n",
    "        print(f\"avarage loss: {np.mean(lossL)}\")\n",
    "        # print(\"Forward pass completed\")\n",
    "        opti.zero_grad()\n",
    "        loss.backward()\n",
    "        draw_dot(loss)\n",
    "        print(loss)\n",
    "        opti.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add 节点 2065752840224 的子节点：[2065750139264, 2065752844208]\n",
      "add 节点 2065753019344 的子节点：[2065753019008, 2065753018960]\n",
      "add 节点 2065753019920 的子节点：[2065753018624, 2065753019680]\n",
      "后序遍历添加节点：2065750139264\n",
      "后序遍历添加节点：2065752844208\n",
      "后序遍历添加节点：2065752840224\n",
      "后序遍历添加节点：2065752843152\n",
      "后序遍历添加节点：2065753018624\n",
      "后序遍历添加节点：2065752694976\n",
      "后序遍历添加节点：2065753018864\n",
      "后序遍历添加节点：2065753019008\n",
      "后序遍历添加节点：2065753018960\n",
      "后序遍历添加节点：2065753019344\n",
      "后序遍历添加节点：2065753019488\n",
      "后序遍历添加节点：2065753019680\n",
      "后序遍历添加节点：2065753019920\n",
      "后序遍历添加节点：2065753020064\n",
      "后序遍历添加节点：2065753020256\n",
      "正确拓扑顺序（子→父）： [2065750139264, 2065752844208, 2065752840224, 2065752843152, 2065753018624, 2065752694976, 2065753018864, 2065753019008, 2065753018960, 2065753019344, 2065753019488, 2065753019680, 2065753019920, 2065753020064, 2065753020256]\n",
      "执行节点 2065753020256 的 _backward\n",
      "执行节点 2065753019920 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065753019680 的 _backward\n",
      "执行节点 2065753019344 的 _backward\n",
      "add 节点的 _backward 被调用！\n",
      "执行节点 2065753019008 的 _backward\n",
      "执行节点 2065753018624 的 _backward\n",
      "执行节点 2065752840224 的 _backward\n",
      "add 节点的 _backward 被调用！\n"
     ]
    }
   ],
   "source": [
    "x1 = Value(5)\n",
    "x2 = Value(2)\n",
    "loss = ((2 + x1)**2 + (4 - x2)**2) / 2\n",
    "loss.backward()\n",
    "# draw_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性层的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Neuron:\n",
    "    def __init__(self, inNum: int) -> None:\n",
    "        # 用NumPy数组存储权重（形状：(in_num,)），每个元素是Value类型\n",
    "        self.weights = np.array([\n",
    "            Value(np.random.uniform(-1, 1)) \n",
    "            for _ in range(inNum)\n",
    "        ])\n",
    "        # 偏置（单个Value）\n",
    "        self.bias = Value(np.random.uniform(-1, 1))\n",
    "    \n",
    "    def __call__(self, x: np.ndarray) -> Value:\n",
    "        # 计算线性部分：z = w·x + b\n",
    "        weighted_sum = np.sum(self.weights * x) + self.bias\n",
    "        return weighted_sum\n",
    "    \n",
    "    def parameters(self):\n",
    "        '''\n",
    "            [w1, w2, ..., b]\n",
    "        '''\n",
    "        return list(self.weights) + [self.bias]\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, inNum: int, outNum: int) -> None:\n",
    "        '''\n",
    "            inNum: 输入特征数\n",
    "            outNum: 神经元个数\n",
    "        '''\n",
    "        self.neurons = np.array([Neuron(inNum) for _ in range(outNum)])\n",
    "    \n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "            前向传播：计算层的输出（所有神经元的输出组成的数组）\n",
    "            out = verctorX @ Layer.W + Layer.b\n",
    "        \"\"\"\n",
    "        # 对每个神经元调用__call__方法，得到输出列表后转为NumPy数组\n",
    "        out = np.array([neuron(x) for neuron in self.neurons])\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        # 遍历每个神经元，收集参数并合并为一个列表\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "    \n",
    "    def zeroGrad(self):\n",
    "        for n in self.parameters():\n",
    "            n.data = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知机"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
