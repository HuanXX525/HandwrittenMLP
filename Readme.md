# 手动实现与优化多层感知机（MLP）课程要求

- 实验目的：深入理解前向传播、反向传播机制以及各种优化算法（SGD, Momentum, Adam）的工作原理。
- 实验内容：不使用深度学习框架的高层 API（如 PyTorch 的 nn.Module），仅使用 NumPy，可以使用自动微分库（如 Autograd）但必须理解反向传播原理。在数据集上训练模型，并比较不同优化器的收敛速度、最终精度和训练稳定性。
- 实验数据集：MNIST

- 评价指标：测试集准确率、训练损失曲线、收敛迭代次数。

- 相关经典论文：Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors.Nature.

## 实验结果与优化器比较

> [!NOTE]
> 待补充

我们对不同优化器（SGD、Momentum、Adam）进行了比较实验，结果如下：

## 本仓库的实现

### 实现的基本原理

本项目手动实现了多层感知机（MLP）的前向传播和反向传播过程，不依赖深度学习框架的高层 API。核心原理包括：

1. **计算图与自动微分**：通过构建计算图来表示神经网络中的各种运算，并实现反向传播算法来自动计算梯度。项目中定义了`Value`类来表示计算图中的节点，每个节点包含数据值和梯度，并通过`_backward`函数定义了局部梯度的计算方式。

2. **前向传播**：数据从输入层经过隐藏层传递到输出层，在每一层中进行线性变换（权重与输入的点积加上偏置）和非线性激活（如 ReLU）。

3. **损失函数**：使用交叉熵损失函数来衡量模型预测值与真实标签之间的差异。

4. **反向传播**：从损失函数开始，通过链式法则逐层计算每个参数的梯度，并将梯度信息传递回网络的每一层。

5. **参数更新**：使用不同的优化算法（如 SGD、Momentum、Adam）根据计算得到的梯度来更新网络参数，以最小化损失函数。

### 基础版本

基础版本实现了最核心的计算图和自动微分机制，使用`Value`类来表示计算图中的节点。每个`Value`对象包含：

- `data`：存储当前节点的数值

- `grad`：存储梯度

- `_child`：存储计算图中的子节点（用于反向传播）

- `_backward`：定义局部梯度计算的函数

该版本支持常见的数学运算（加法、减法、乘法、除法、幂运算、指数、对数等）和激活函数（ReLU、Softmax）。神经网络的层（`Layer`）由多个神经元（`Neuron`）组成，每个神经元包含权重和偏置参数。

优化器实现了三种算法：

1. **SGD**：随机梯度下降，直接使用负梯度方向更新参数

2. **Momentum**：带动量的 SGD，利用历史梯度信息加速收敛并抑制震荡

3. **Adam**：结合动量和自适应学习率的优化算法，适用于大多数场景

### Numpy 加速版本

Numpy 加速版本在基础版本的基础上进行了优化，主要改进包括：

1. **向量化操作**：将单个数值的运算扩展到向量和矩阵操作，利用 Numpy 的高效数组运算能力，大幅提升计算效率。在加速版本中，`Value`类的`data`属性不再是单个数值，而是 Numpy 数组，支持批量处理多个数据。

2. **广播机制**：支持数组间的广播运算，使得不同形状的数组能够进行运算，增强了计算的灵活性。

3. **改进的层实现**：`Layer`类直接使用向量化的权重矩阵和偏置向量，通过矩阵乘法实现高效的线性变换，而不是使用循环遍历单个神经元。

4. **优化的激活函数**：激活函数也进行了向量化实现，能够对整个向量或矩阵进行操作，而不是逐个元素处理。

通过这些优化，Numpy 加速版本在保持计算图和自动微分功能的同时，显著提高了计算性能，使得模型能够处理更大规模的数据集和更复杂的网络结构。
